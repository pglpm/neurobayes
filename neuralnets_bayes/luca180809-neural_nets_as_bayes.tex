\pdfoutput=1
%% Author: PGL  Porta Mana
%% Created: 2018-08-07T20:36:31+0200
%% Last-Updated: 2019-01-18T22:05:49+0100
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Report-no: ***
\newif\ifarxiv
\arxivfalse
\ifarxiv\pdfmapfile{+classico.map}\fi
\newif\ifafour
\afourfalse % true = A4, false = A5
\newif\iftypodisclaim % typographical disclaim on the side
\typodisclaimtrue
\newif\ifpublic
\publictrue % true = for publication, false = personal notes
\newcommand*{\memfontfamily}{zplx}
\newcommand*{\memfontpack}{newpxtext}
\documentclass[\ifafour a4paper,12pt,\else a5paper,10pt,\fi%extrafontsizes,%
onecolumn,oneside,article,%french,italian,german,swedish,latin,
british%
]{memoir}
\newcommand*{\updated}{\today}
\newcommand*{\firstdraft}{9 August 2018}
\newcommand*{\firstpublished}{\today}
\newcommand*{\propertitle}{Neural networks, probability, decision theory}
\newcommand*{\pdftitle}{\propertitle}
\newcommand*{\headtitle}{Neural nets, probability, decision theory}
\newcommand*{\pdfauthor}{P.G.L.  Porta Mana, I. A. Davidovich}
\newcommand*{\headauthor}{\ifpublic Davidovich, Porta Mana%
\else Luca\fi}
\newcommand*{\reporthead}{}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\usepackage{pifont}
%\usepackage{fontawesome}
\usepackage[T1]{fontenc} 
\input{glyphtounicode} \pdfgentounicode=1
\usepackage[utf8]{inputenx}
%\usepackage{newunicodechar}
% \newunicodechar{Ĕ}{\u{E}}
% \newunicodechar{ĕ}{\u{e}}
% \newunicodechar{Ĭ}{\u{I}}
% \newunicodechar{ĭ}{\u{\i}}
% \newunicodechar{Ŏ}{\u{O}}
% \newunicodechar{ŏ}{\u{o}}
% \newunicodechar{Ŭ}{\u{U}}
% \newunicodechar{ŭ}{\u{u}}
% \newunicodechar{Ā}{\=A}
% \newunicodechar{ā}{\=a}
% \newunicodechar{Ē}{\=E}
% \newunicodechar{ē}{\=e}
% \newunicodechar{Ī}{\=I}
% \newunicodechar{ī}{\={\i}}
% \newunicodechar{Ō}{\=O}
% \newunicodechar{ō}{\=o}
% \newunicodechar{Ū}{\=U}
% \newunicodechar{ū}{\=u}
% \newunicodechar{Ȳ}{\=Y}
% \newunicodechar{ȳ}{\=y}

\newcommand*{\bmmax}{0} % reduce number of bold fonts, before bm
\newcommand*{\hmmax}{0} % reduce number of heavy fonts, before bm
\usepackage{textcomp}
\usepackage[normalem]{ulem}
% \makeatletter
% \def\ssout{\bgroup \ULdepth=-.35ex%\UL@setULdepth
%  \markoverwith{\lower\ULdepth\hbox
%    {\kern-.03em\vbox{\hrule width.2em\kern1.2\p@\hrule}\kern-.03em}}%
%  \ULon}
% \makeatother
\usepackage{amsmath}
\usepackage{mathtools}
\addtolength{\jot}{\jot} % increase spacing in multiline formulae
\usepackage{empheq}% automatically calls amsmath and mathtools
\newcommand*{\widefbox}[1]{\fbox{\hspace{1em}#1\hspace{1em}}}
\setlength{\multlinegap}{0pt}
%\usepackage{fancybox}
%\usepackage{framed}
% \usepackage[misc]{ifsym} % for dice
% \newcommand*{\diceone}{{\scriptsize\Cube{1}}}
\usepackage{amssymb}
\usepackage{amsxtra}

\usepackage[main=british,french,italian,german,swedish,latin,esperanto]{babel}\selectlanguage{british}
\newcommand*{\langfrench}{\foreignlanguage{french}}
\newcommand*{\langgerman}{\foreignlanguage{german}}
\newcommand*{\langitalian}{\foreignlanguage{italian}}
\newcommand*{\langswedish}{\foreignlanguage{swedish}}
\newcommand*{\langlatin}{\foreignlanguage{latin}}
\newcommand*{\langnohyph}{\foreignlanguage{nohyphenation}}

\usepackage[autostyle=false,autopunct=false,english=british]{csquotes}
\setquotestyle{british}

\usepackage{amsthm}
\newcommand*{\QED}{\textsc{q.e.d.}}
\renewcommand*{\qedsymbol}{\QED}
\theoremstyle{remark}
\newtheorem{note}{Note}
\newtheorem*{remark}{Note}
\newtheoremstyle{innote}{\parsep}{\parsep}{\footnotesize}{}{}{}{0pt}{}
\theoremstyle{innote}
\newtheorem*{innote}{}


\usepackage[shortlabels,inline]{enumitem}
\SetEnumitemKey{para}{itemindent=\parindent,leftmargin=0pt,parsep=0pt,itemsep=\topsep}
% \begin{asparaenum} = \begin{enumerate}[para]
% \begin{inparaenum} = \begin{enumerate*}
%\setlist[enumerate,2]{label=\alph*.}
%\setlist[enumerate]{leftmargin=0.5\parindent,labelindent=0pt,itemindent=0.5\parindent,labelsep=!,align=left}
%\setlist[itemize]{leftmargin=0.5\parindent,labelindent=0pt,itemindent=0.5\parindent,labelsep=!,align=left}
%\setlist[description]{leftmargin=0.5\parindent,labelindent=0pt,labelsep=!,align=left}

\usepackage[babel,theoremfont]{newpxtext}
\usepackage[bigdelims,nosymbolsc%,smallerops % probably arXiv doesn't have it
]{newpxmath}
\useosf\linespread{1.083}
%% smaller operators for old version of newpxmath
\makeatletter
\def\re@DeclareMathSymbol#1#2#3#4{%
    \let#1=\undefined
    \DeclareMathSymbol{#1}{#2}{#3}{#4}}
%\re@DeclareMathSymbol{\bigsqcupop}{\mathop}{largesymbols}{"46}
%\re@DeclareMathSymbol{\bigodotop}{\mathop}{largesymbols}{"4A}
\re@DeclareMathSymbol{\bigoplusop}{\mathop}{largesymbols}{"4C}
\re@DeclareMathSymbol{\bigotimesop}{\mathop}{largesymbols}{"4E}
\re@DeclareMathSymbol{\sumop}{\mathop}{largesymbols}{"50}
\re@DeclareMathSymbol{\prodop}{\mathop}{largesymbols}{"51}
\re@DeclareMathSymbol{\bigcupop}{\mathop}{largesymbols}{"53}
\re@DeclareMathSymbol{\bigcapop}{\mathop}{largesymbols}{"54}
%\re@DeclareMathSymbol{\biguplusop}{\mathop}{largesymbols}{"55}
\re@DeclareMathSymbol{\bigwedgeop}{\mathop}{largesymbols}{"56}
\re@DeclareMathSymbol{\bigveeop}{\mathop}{largesymbols}{"57}
%\re@DeclareMathSymbol{\bigcupdotop}{\mathop}{largesymbols}{"DF}
%\re@DeclareMathSymbol{\bigcapplusop}{\mathop}{largesymbolsPXA}{"00}
%\re@DeclareMathSymbol{\bigsqcupplusop}{\mathop}{largesymbolsPXA}{"02}
%\re@DeclareMathSymbol{\bigsqcapplusop}{\mathop}{largesymbolsPXA}{"04}
%\re@DeclareMathSymbol{\bigsqcapop}{\mathop}{largesymbolsPXA}{"06}
\re@DeclareMathSymbol{\bigtimesop}{\mathop}{largesymbolsPXA}{"10}
%\re@DeclareMathSymbol{\coprodop}{\mathop}{largesymbols}{"60}
%\re@DeclareMathSymbol{\varprod}{\mathop}{largesymbolsPXA}{16}
\makeatother


%% With euler font cursive for Greek letters - the [1] means 100% scaling
\DeclareFontFamily{U}{egreek}{\skewchar\font'177}%
\DeclareFontShape{U}{egreek}{m}{n}{<-6>s*[1]eurm5 <6-8>s*[1]eurm7 <8->s*[1]eurm10}{}%
\DeclareFontShape{U}{egreek}{m}{it}{<->s*[1]eurmo10}{}%
\DeclareFontShape{U}{egreek}{b}{n}{<-6>s*[1]eurb5 <6-8>s*[1]eurb7 <8->s*[1]eurb10}{}%
\DeclareFontShape{U}{egreek}{b}{it}{<->s*[1]eurbo10}{}%
\DeclareSymbolFont{egreeki}{U}{egreek}{m}{it}%
\SetSymbolFont{egreeki}{bold}{U}{egreek}{b}{it}% from the amsfonts package
\DeclareSymbolFont{egreekr}{U}{egreek}{m}{n}%
\SetSymbolFont{egreekr}{bold}{U}{egreek}{b}{n}% from the amsfonts package
% Take also \sum, \prod, \coprod symbols from Euler fonts
\DeclareFontFamily{U}{egreekx}{\skewchar\font'177}
\DeclareFontShape{U}{egreekx}{m}{n}{%
       <-7.5>s*[0.9]euex7%
    <7.5-8.5>s*[0.9]euex8%
    <8.5-9.5>s*[0.9]euex9%
    <9.5->s*[0.9]euex10%
}{}
\DeclareSymbolFont{egreekx}{U}{egreekx}{m}{n}
\DeclareMathSymbol{\sumop}{\mathop}{egreekx}{"50}
\DeclareMathSymbol{\prodop}{\mathop}{egreekx}{"51}
\DeclareMathSymbol{\coprodop}{\mathop}{egreekx}{"60}
\makeatletter
\def\sum{\DOTSI\sumop\slimits@}
\def\prod{\DOTSI\prodop\slimits@}
\def\coprod{\DOTSI\coprodop\slimits@}
\makeatother
\ifarxiv\else\input{../undefinegreek.tex}\fi% make sure no CMF greek letters sneak in
% Greek letters not usually given in LaTeX. Comment the unneeded ones
% \DeclareMathSymbol{\varpartial}{\mathalpha}{egreeki}{"40}
 \DeclareMathSymbol{\partialup}{\mathalpha}{egreekr}{"40}
% \DeclareMathSymbol{\alpha}{\mathalpha}{egreeki}{"0B}
% \DeclareMathSymbol{\beta}{\mathalpha}{egreeki}{"0C}
% \DeclareMathSymbol{\gamma}{\mathalpha}{egreeki}{"0D}
% \DeclareMathSymbol{\delta}{\mathalpha}{egreeki}{"0E}
 \DeclareMathSymbol{\epsilon}{\mathalpha}{egreeki}{"0F}
% \DeclareMathSymbol{\zeta}{\mathalpha}{egreeki}{"10}
% \DeclareMathSymbol{\eta}{\mathalpha}{egreeki}{"11}
% \DeclareMathSymbol{\theta}{\mathalpha}{egreeki}{"12}
% \DeclareMathSymbol{\iota}{\mathalpha}{egreeki}{"13}
 \DeclareMathSymbol{\kappa}{\mathalpha}{egreeki}{"14}
 \DeclareMathSymbol{\lambda}{\mathalpha}{egreeki}{"15}
% \DeclareMathSymbol{\mu}{\mathalpha}{egreeki}{"16}
 \DeclareMathSymbol{\nu}{\mathalpha}{egreeki}{"17}
% \DeclareMathSymbol{\xi}{\mathalpha}{egreeki}{"18}
% \DeclareMathSymbol{\omicron}{\mathalpha}{egreeki}{"6F}
% \DeclareMathSymbol{\pi}{\mathalpha}{egreeki}{"19}
% \DeclareMathSymbol{\rho}{\mathalpha}{egreeki}{"1A}
% \DeclareMathSymbol{\sigma}{\mathalpha}{egreeki}{"1B}
% \DeclareMathSymbol{\tau}{\mathalpha}{egreeki}{"1C}
% \DeclareMathSymbol{\upsilon}{\mathalpha}{egreeki}{"1D}
% \DeclareMathSymbol{\phi}{\mathalpha}{egreeki}{"1E}
% \DeclareMathSymbol{\chi}{\mathalpha}{egreeki}{"1F}
% \DeclareMathSymbol{\psi}{\mathalpha}{egreeki}{"20}
% \DeclareMathSymbol{\omega}{\mathalpha}{egreeki}{"21}
% \DeclareMathSymbol{\varepsilon}{\mathalpha}{egreeki}{"22}
% \DeclareMathSymbol{\vartheta}{\mathalpha}{egreeki}{"23}
% \DeclareMathSymbol{\varpi}{\mathalpha}{egreeki}{"24}
% \let\varrho\rho 
% \let\varsigma\sigma
 \let\varkappa\kappa
% \DeclareMathSymbol{\varphi}{\mathalpha}{egreeki}{"27}
% %
% \DeclareMathSymbol{\varAlpha}{\mathalpha}{egreeki}{"41}
% \DeclareMathSymbol{\varBeta}{\mathalpha}{egreeki}{"42}
% \DeclareMathSymbol{\varGamma}{\mathalpha}{egreeki}{"00}
 \DeclareMathSymbol{\varDelta}{\mathalpha}{egreeki}{"01}
 \DeclareMathSymbol{\varEpsilon}{\mathalpha}{egreeki}{"45}
% \DeclareMathSymbol{\varZeta}{\mathalpha}{egreeki}{"5A}
 \DeclareMathSymbol{\varEta}{\mathalpha}{egreeki}{"48}
% \DeclareMathSymbol{\varTheta}{\mathalpha}{egreeki}{"02}
 \DeclareMathSymbol{\varIota}{\mathalpha}{egreeki}{"49}
% \DeclareMathSymbol{\varKappa}{\mathalpha}{egreeki}{"4B}
% \DeclareMathSymbol{\varLambda}{\mathalpha}{egreeki}{"03}
 \DeclareMathSymbol{\varMu}{\mathalpha}{egreeki}{"4D}
% \DeclareMathSymbol{\varNu}{\mathalpha}{egreeki}{"4E}
% \DeclareMathSymbol{\varXi}{\mathalpha}{egreeki}{"04}
% \DeclareMathSymbol{\varOmicron}{\mathalpha}{egreeki}{"4F}
% \DeclareMathSymbol{\varPi}{\mathalpha}{egreeki}{"05}
% \DeclareMathSymbol{\varRho}{\mathalpha}{egreeki}{"50}
% \DeclareMathSymbol{\varSigma}{\mathalpha}{egreeki}{"06}
% \DeclareMathSymbol{\varTau}{\mathalpha}{egreeki}{"54}
% \DeclareMathSymbol{\varUpsilon}{\mathalpha}{egreeki}{"07}
% \DeclareMathSymbol{\varPhi}{\mathalpha}{egreeki}{"08}
% \DeclareMathSymbol{\varChi}{\mathalpha}{egreeki}{"58}
% \DeclareMathSymbol{\varPsi}{\mathalpha}{egreeki}{"09}
% \DeclareMathSymbol{\varOmega}{\mathalpha}{egreeki}{"0A} 
% %
% \DeclareMathSymbol{\Alpha}{\mathalpha}{egreekr}{"41}
% \DeclareMathSymbol{\Beta}{\mathalpha}{egreekr}{"42}
% \DeclareMathSymbol{\Gamma}{\mathalpha}{egreekr}{"00}
% \DeclareMathSymbol{\Delta}{\mathalpha}{egreekr}{"01}
% \DeclareMathSymbol{\Epsilon}{\mathalpha}{egreekr}{"45}
% \DeclareMathSymbol{\Zeta}{\mathalpha}{egreekr}{"5A}
% \DeclareMathSymbol{\Eta}{\mathalpha}{egreekr}{"48}
% \DeclareMathSymbol{\Theta}{\mathalpha}{egreekr}{"02}
% \DeclareMathSymbol{\Iota}{\mathalpha}{egreekr}{"49}
% \DeclareMathSymbol{\Kappa}{\mathalpha}{egreekr}{"4B}
% \DeclareMathSymbol{\Lambda}{\mathalpha}{egreekr}{"03}
% \DeclareMathSymbol{\Mu}{\mathalpha}{egreekr}{"4D}
% \DeclareMathSymbol{\Nu}{\mathalpha}{egreekr}{"4E}
% \DeclareMathSymbol{\Xi}{\mathalpha}{egreekr}{"04}
% \DeclareMathSymbol{\Omicron}{\mathalpha}{egreekr}{"4F}
% \DeclareMathSymbol{\Pi}{\mathalpha}{egreekr}{"05}
% \DeclareMathSymbol{\Rho}{\mathalpha}{egreekr}{"50}
% \DeclareMathSymbol{\Sigma}{\mathalpha}{egreekr}{"06}
% \DeclareMathSymbol{\Tau}{\mathalpha}{egreekr}{"54}
% \DeclareMathSymbol{\Upsilon}{\mathalpha}{egreekr}{"07}
% \DeclareMathSymbol{\Phi}{\mathalpha}{egreekr}{"08}
% \DeclareMathSymbol{\Chi}{\mathalpha}{egreekr}{"58}
% \DeclareMathSymbol{\Psi}{\mathalpha}{egreekr}{"09}
% \DeclareMathSymbol{\Omega}{\mathalpha}{egreekr}{"0A}
% %
% \DeclareMathSymbol{\alphaup}{\mathalpha}{egreekr}{"0B}
% \DeclareMathSymbol{\betaup}{\mathalpha}{egreekr}{"0C}
% \DeclareMathSymbol{\gammaup}{\mathalpha}{egreekr}{"0D}
 \DeclareMathSymbol{\deltaup}{\mathalpha}{egreekr}{"0E}
% \DeclareMathSymbol{\epsilonup}{\mathalpha}{egreekr}{"0F}
% \DeclareMathSymbol{\zetaup}{\mathalpha}{egreekr}{"10}
% \DeclareMathSymbol{\etaup}{\mathalpha}{egreekr}{"11}
% \DeclareMathSymbol{\thetaup}{\mathalpha}{egreekr}{"12}
% \DeclareMathSymbol{\iotaup}{\mathalpha}{egreekr}{"13}
% \DeclareMathSymbol{\kappaup}{\mathalpha}{egreekr}{"14}
% \DeclareMathSymbol{\lambdaup}{\mathalpha}{egreekr}{"15}
% \DeclareMathSymbol{\muup}{\mathalpha}{egreekr}{"16}
% \DeclareMathSymbol{\nuup}{\mathalpha}{egreekr}{"17}
% \DeclareMathSymbol{\xiup}{\mathalpha}{egreekr}{"18}
% \DeclareMathSymbol{\omicronup}{\mathalpha}{egreekr}{"6F}
  \DeclareMathSymbol{\piup}{\mathalpha}{egreekr}{"19}
% \DeclareMathSymbol{\rhoup}{\mathalpha}{egreekr}{"1A}
% \DeclareMathSymbol{\sigmaup}{\mathalpha}{egreekr}{"1B}
% \DeclareMathSymbol{\tauup}{\mathalpha}{egreekr}{"1C}
% \DeclareMathSymbol{\upsilonup}{\mathalpha}{egreekr}{"1D}
% \DeclareMathSymbol{\phiup}{\mathalpha}{egreekr}{"1E}
% \DeclareMathSymbol{\chiup}{\mathalpha}{egreekr}{"1F}
% \DeclareMathSymbol{\psiup}{\mathalpha}{egreekr}{"20}
% \DeclareMathSymbol{\omegaup}{\mathalpha}{egreekr}{"21}
% \DeclareMathSymbol{\varepsilonup}{\mathalpha}{egreekr}{"22}
% \DeclareMathSymbol{\varthetaup}{\mathalpha}{egreekr}{"23}
% \DeclareMathSymbol{\varpiup}{\mathalpha}{egreekr}{"24}
% \let\varrhoup\rhoup 
% \let\varsigmaup\sigmaup
% \let\varkappaup\kappaup
% \DeclareMathSymbol{\varphiup}{\mathalpha}{egreekr}{"27}

% Optima as sans-serif font
%\usepackage%[scaled=0.9]%
%{classico}
\renewcommand\sfdefault{uop}
\DeclareMathAlphabet{\mathsf}  {T1}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsf}{bold}{T1}{\sfdefault}{b}{sl}
\newcommand*{\mathte}[1]{\textbf{\textit{\textsf{#1}}}}
% Upright sans-serif math alphabet
% \DeclareMathAlphabet{\mathsu}  {T1}{\sfdefault}{m}{n}
% \SetMathAlphabet{\mathsu}{bold}{T1}{\sfdefault}{b}{n}

% DejaVu Mono as typewriter text
\usepackage[scaled=0.84]{DejaVuSansMono}


\usepackage{mathdots}

\usepackage[usenames]{xcolor}
% Tol (2012) colour-blind-, print-, screen-friendly colours, alternative scheme; Munsell terminology
\definecolor{mypurpleblue}{RGB}{68,119,170}
\definecolor{myblue}{RGB}{102,204,238}
\definecolor{mygreen}{RGB}{34,136,51}
\definecolor{myyellow}{RGB}{204,187,68}
\definecolor{myred}{RGB}{238,102,119}
\definecolor{myredpurple}{RGB}{170,51,119}
\definecolor{mygrey}{RGB}{187,187,187}

% Tol (2012) colour-blind-, print-, screen-friendly colours; Munsell terminology
% \definecolor{lbpurple}{RGB}{51,34,136}
% \definecolor{lblue}{RGB}{136,204,238}
% \definecolor{lbgreen}{RGB}{68,170,153}
% \definecolor{lgreen}{RGB}{17,119,51}
% \definecolor{lgyellow}{RGB}{153,153,51}
% \definecolor{lyellow}{RGB}{221,204,119}
% \definecolor{lred}{RGB}{204,102,119}
% \definecolor{lpred}{RGB}{136,34,85}
% \definecolor{lrpurple}{RGB}{170,68,153}
 \definecolor{lgrey}{RGB}{221,221,221}
%\newcommand*\mycolourbox[1]{%
%\colorbox{mygrey}{\hspace{1em}#1\hspace{1em}}}
\colorlet{shadecolor}{lgrey}

\usepackage{bm}
\usepackage{microtype}

\usepackage[backend=biber,mcite,%subentry,
citestyle=authoryear-comp,bibstyle=pglpm-authoryear,autopunct=false,sorting=ny,sortcites=false,natbib=false,maxcitenames=1,maxbibnames=8,minbibnames=8,giveninits=true,uniquename=false,uniquelist=false,maxalphanames=1,block=space,hyperref=true,defernumbers=false,useprefix=true,sortupper=false,language=british,parentracker=false]{biblatex}
\DeclareSortingScheme{ny}{\sort{\field{sortname}\field{author}\field{editor}}\sort{\field{year}}}
\iffalse\makeatletter%%% replace parenthesis with brackets
\newrobustcmd*{\parentexttrack}[1]{%
  \begingroup
  \blx@blxinit
  \blx@setsfcodes
  \blx@bibopenparen#1\blx@bibcloseparen
  \endgroup}
\AtEveryCite{%
  \let\parentext=\parentexttrack%
  \let\bibopenparen=\bibopenbracket%
  \let\bibcloseparen=\bibclosebracket}
\makeatother\fi
\DefineBibliographyExtras{british}{\def\finalandcomma{\addcomma}}
\renewcommand*{\finalnamedelim}{\addcomma\space}
\setcounter{biburlnumpenalty}{1}
\setcounter{biburlucpenalty}{0}
\setcounter{biburllcpenalty}{1}
\DeclareDelimFormat{multicitedelim}{\addsemicolon\space}
\DeclareDelimFormat{compcitedelim}{\addsemicolon\space}
\DeclareDelimFormat{postnotedelim}{\space}
\ifarxiv\else\addbibresource{../portamanabib.bib}\fi
\renewcommand{\bibfont}{\footnotesize}
%\appto{\citesetup}{\footnotesize}% smaller font for citations
\defbibheading{bibliography}[\bibname]{\section*{#1}\addcontentsline{toc}{section}{#1}%\markboth{#1}{#1}
}
\newcommand*{\citep}{\parencites}
\newcommand*{\citey}{\parencites*}
%\renewcommand*{\cite}{\parencite}
\renewcommand*{\cites}{\parencites}
\providecommand{\href}[2]{#2}
\providecommand{\eprint}[2]{\texttt{\href{#1}{#2}}}
\newcommand*{\amp}{\&}
% \newcommand*{\citein}[2][]{\textnormal{\textcite[#1]{#2}}%\addtocategory{extras}{#2}
% }
\newcommand*{\citein}[2][]{\textnormal{\textcite[#1]{#2}}%\addtocategory{extras}{#2}
}
\newcommand*{\citebi}[2][]{\textcite[#1]{#2}%\addtocategory{extras}{#2}
}
\newcommand*{\subtitleproc}[1]{}
\newcommand*{\chapb}{ch.}

% \def\arxivp{}
% \def\mparcp{}
% \def\philscip{}
% \def\biorxivp{}
% \newcommand*{\arxivsi}{\texttt{arXiv} eprints available at \url{http://arxiv.org/}.\\}
% \newcommand*{\mparcsi}{\texttt{mp\_arc} eprints available at \url{http://www.ma.utexas.edu/mp_arc/}.\\}
% \newcommand*{\philscisi}{\texttt{philsci} eprints available at \url{http://philsci-archive.pitt.edu/}.\\}
% \newcommand*{\biorxivsi}{\texttt{bioRxiv} eprints available at \url{http://biorxiv.org/}.\\}
\newcommand*{\arxiveprint}[1]{%\global\def\arxivp{\arxivsi}%\citeauthor{0arxivcite}\addtocategory{ifarchcit}{0arxivcite}%eprint
\texttt{\urlalt{https://arxiv.org/abs/#1}{arXiv:\hspace{0pt}#1}}%
%\texttt{\href{http://arxiv.org/abs/#1}{\protect\url{arXiv:#1}}}%
%\renewcommand{\arxivnote}{\texttt{arXiv} eprints available at \url{http://arxiv.org/}.}
}
\newcommand*{\mparceprint}[1]{%\global\def\mparcp{\mparcsi}%\citeauthor{0mparccite}\addtocategory{ifarchcit}{0mparccite}%eprint
\texttt{\urlalt{http://www.ma.utexas.edu/mp_arc-bin/mpa?yn=#1}{mp\_arc:\hspace{0pt}#1}}%
%\texttt{\href{http://www.ma.utexas.edu/mp_arc-bin/mpa?yn=#1}{\protect\url{mp_arc:#1}}}%
%\providecommand{\mparcnote}{\texttt{mp_arc} eprints available at \url{http://www.ma.utexas.edu/mp_arc/}.}
}
\newcommand*{\philscieprint}[1]{%\global\def\philscip{\philscisi}%\citeauthor{0philscicite}\addtocategory{ifarchcit}{0philscicite}%eprint
\texttt{\urlalt{http://philsci-archive.pitt.edu/archive/#1}{PhilSci:\hspace{0pt}#1}}%
%\texttt{\href{http://philsci-archive.pitt.edu/archive/#1}{\protect\url{PhilSci:#1}}}%
%\providecommand{\mparcnote}{\texttt{philsci} eprints available at \url{http://philsci-archive.pitt.edu/}.}
}
\newcommand*{\biorxiveprint}[1]{%\global\def\biorxivp{\biorxivsi}%\citeauthor{0arxivcite}\addtocategory{ifarchcit}{0arxivcite}%eprint
\texttt{\urlalt{https://doi.org/10.1101/#1}{bioRxiv doi:\hspace{0pt}10.1101/#1}}%
%\texttt{\href{http://arxiv.org/abs/#1}{\protect\url{arXiv:#1}}}%
%\renewcommand{\arxivnote}{\texttt{arXiv} eprints available at \url{http://arxiv.org/}.}
}
\newcommand*{\osfeprint}[1]{%
\texttt{\urlalt{https://doi.org/10.17605/osf.io/#1}{Open Science Framework doi:10.17605/osf.io/#1}}%
}

\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{tikz-cd}

\PassOptionsToPackage{hyphens}{url}\usepackage[hypertexnames=false]{hyperref}
\usepackage[depth=4]{bookmark}
\hypersetup{colorlinks=true,bookmarksnumbered,pdfborder={0 0 0.25},citebordercolor={0.2667 0.4667 0.6667},citecolor=mypurpleblue,linkbordercolor={0.6667 0.2 0.4667},linkcolor=myredpurple,urlbordercolor={0.1333 0.5333 0.2},urlcolor=mygreen,breaklinks=true,pdftitle={\pdftitle},pdfauthor={\pdfauthor}}
% \usepackage[vertfit=local]{breakurl}% only for arXiv
\providecommand*{\urlalt}{\href}

%%% Layout. I do not know on which kind of paper the reader will print the
%%% paper on (A4? letter? one-sided? double-sided?). So I choose A5, which
%%% provides a good layout for reading on screen and save paper if printed
%%% two pages per sheet. Average length line is 66 characters and page
%%% numbers are centred.
\ifafour\setstocksize{297mm}{210mm}%{*}% A4
\else\setstocksize{210mm}{5.5in}%{*}% 210x139.7
\fi
\settrimmedsize{\stockheight}{\stockwidth}{*}
\setlxvchars[\normalfont] %313.3632pt for a 66-characters line
\setxlvchars[\normalfont]
\setlength{\trimtop}{0pt}
\setlength{\trimedge}{\stockwidth}
\addtolength{\trimedge}{-\paperwidth}
% The length of the normalsize alphabet is 133.05988pt - 10 pt = 26.1408pc
% The length of the normalsize alphabet is 159.6719pt - 12pt = 30.3586pc
% Bringhurst gives 32pc as boundary optimal with 69 ch per line
% The length of the normalsize alphabet is 191.60612pt - 14pt = 35.8634pc
\ifafour\settypeblocksize{*}{32pc}{1.618} % A4
%\setulmargins{*}{*}{1.667}%gives 5/3 margins % 2 or 1.667
\else\settypeblocksize{*}{26pc}{1.618}% nearer to a 66-line newpx and preserves GR
\fi
\setulmargins{*}{*}{1}%gives equal margins
\setlrmargins{*}{*}{*}
\setheadfoot{\onelineskip}{2.5\onelineskip}
\setheaderspaces{*}{2\onelineskip}{*}
\setmarginnotes{2ex}{10mm}{0pt}
\checkandfixthelayout[nearest]
\fixpdflayout
%%% End layout
%% this fixes missing white spaces
\pdfmapline{+dummy-space <dummy-space.pfb}\pdfinterwordspaceon%

%%% Sectioning
\newcommand*{\asudedication}[1]{%
{\par\centering\textit{#1}\par}}
\newenvironment{acknowledgements}{\section*{Thanks}\addcontentsline{toc}{section}{Thanks}}{\par}
\makeatletter\renewcommand{\appendix}{\par
  \bigskip{\centering
   \interlinepenalty \@M
   \normalfont
   \printchaptertitle{\sffamily\appendixpagename}\par}
  \setcounter{section}{0}%
  \gdef\@chapapp{\appendixname}%
  \gdef\thesection{\@Alph\c@section}%
  \anappendixtrue}\makeatother
\counterwithout{section}{chapter}
\setsecnumformat{\upshape\csname the#1\endcsname\quad}
\setsecheadstyle{\large\bfseries\sffamily%
\raggedright}
\setsubsecheadstyle{\bfseries\sffamily%
\raggedright}
%\setbeforesecskip{-1.5ex plus 1ex minus .2ex}% plus 1ex minus .2ex}
%\setaftersecskip{1.3ex plus .2ex }% plus 1ex minus .2ex}
%\setsubsubsecheadstyle{\bfseries\sffamily\slshape\raggedright}
%\setbeforesubsecskip{1.25ex plus 1ex minus .2ex }% plus 1ex minus .2ex}
%\setaftersubsecskip{-1em}%{-0.5ex plus .2ex}% plus 1ex minus .2ex}
\setsubsecindent{0pt}%0ex plus 1ex minus .2ex}
\setparaheadstyle{\bfseries\sffamily%
\raggedright}
\setcounter{secnumdepth}{2}
\setlength{\headwidth}{\textwidth}
\newcommand{\addchap}[1]{\chapter*[#1]{#1}\addcontentsline{toc}{chapter}{#1}}
\newcommand{\addsec}[1]{\section*{#1}\addcontentsline{toc}{section}{#1}}
\newcommand{\addsubsec}[1]{\subsection*{#1}\addcontentsline{toc}{subsection}{#1}}
\newcommand{\addpara}[1]{\paragraph*{#1.}\addcontentsline{toc}{subsubsection}{#1}}
\newcommand{\addparap}[1]{\paragraph*{#1}\addcontentsline{toc}{subsubsection}{#1}}

% Headers and footers
\copypagestyle{manaart}{plain}
\makeheadrule{manaart}{\headwidth}{0.5\normalrulethickness}
\makeoddhead{manaart}{%
{\footnotesize%\sffamily%
\scshape\headauthor}}{}{{\footnotesize\sffamily%
\headtitle}}
\makeoddfoot{manaart}{}{\thepage}{}
\newcommand*\autanet{\includegraphics[height=\heightof{M}]{../autanet.pdf}}
\definecolor{mygray}{gray}{0.333}
\iftypodisclaim%
\ifafour\newcommand\addprintnote{\begin{picture}(0,0)%
\put(245,149){\makebox(0,0){\rotatebox{90}{\tiny\color{mygray}\textsf{This
            document is designed for screen reading and
            two-up printing on A4 or Letter paper}}}}%
\end{picture}}% A4
\else\newcommand\addprintnote{\begin{picture}(0,0)%
\put(176,112){\makebox(0,0){\rotatebox{90}{\tiny\color{mygray}\textsf{This
            document is designed for screen reading and
            two-up printing on A4 or Letter paper}}}}%
\end{picture}}\fi%afourtrue
\makeoddfoot{plain}{}{\makebox[0pt]{\thepage}\addprintnote}{}
\else
\makeoddfoot{plain}{}{\makebox[0pt]{\thepage}}{}
\fi%typodisclaimtrue
\makeoddhead{plain}{}{}{\footnotesize\reporthead}

% \copypagestyle{manainitial}{plain}
% \makeheadrule{manainitial}{\headwidth}{0.5\normalrulethickness}
% \makeoddhead{manainitial}{%
% \footnotesize\sffamily%
% \scshape\headauthor}{}{\footnotesize\sffamily%
% \headtitle}
% \makeoddfoot{manaart}{}{\thepage}{}

\pagestyle{manaart}

\setlength{\droptitle}{-3.9\onelineskip}
\pretitle{\begin{center}\Large\sffamily%
\bfseries}
\posttitle{\bigskip\end{center}}

\makeatletter\newcommand*{\atf}{\includegraphics[%trim=1pt 1pt 0pt 0pt,
totalheight=\heightof{@}]{../atblack.png}}\makeatother
\providecommand{\affiliation}[1]{\textsl{\textsf{\footnotesize #1}}}
\providecommand{\epost}[1]{\texttt{\footnotesize\textless#1\textgreater}}
\providecommand{\email}[2]{\href{mailto:#1ZZ@#2 ((remove ZZ))}{#1\protect\atf#2}}

\preauthor{\vspace{-0.5\baselineskip}\begin{center}
\normalsize\sffamily%
\lineskip  0.5em}
\postauthor{\par\end{center}}
\predate{\DTMsetdatestyle{mydate}\begin{center}\footnotesize}
\postdate{\end{center}\vspace{-\medskipamount}}
\usepackage[british]{datetime2}
\DTMnewdatestyle{mydate}%
{% definitions
\renewcommand*{\DTMdisplaydate}[4]{%
\number##3\ \DTMenglishmonthname{##2} ##1}%
\renewcommand*{\DTMDisplaydate}{\DTMdisplaydate}%
}
\DTMsetdatestyle{mydate}


\setfloatadjustment{figure}{\footnotesize}
\captiondelim{\quad}
\captionnamefont{\footnotesize\sffamily%
}
\captiontitlefont{\footnotesize}
\firmlists*
\midsloppy

% handling orphan/widow lines, memman.pdf
% \clubpenalty=10000
% \widowpenalty=10000
% \raggedbottom
% Downes, memman.pdf
\clubpenalty=9996
\widowpenalty=9999
\brokenpenalty=4991
\predisplaypenalty=10000
\postdisplaypenalty=1549
\displaywidowpenalty=1602

\selectlanguage{british}\frenchspacing
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%% Paper's details %%%%
\title{\propertitle%\\
%  {\large A geometric commentary on maximum-entropy proofs}% ***
}
\author{%
\hspace*{\stretch{1}}%
%% uncomment if additional authors present
\parbox{0.5\linewidth}%\makebox[0pt][c]%
{\protect\centering I. A. Davidovich\\%
\footnotesize\epost{\email{ivan.davidovich}{ntnu.no}}}%
\hspace*{\stretch{1}}%
\parbox{0.5\linewidth}%\makebox[0pt][c]%
{\protect\centering\ifpublic P.G.L.  Porta Mana\\\else Luca  \fi%
\footnotesize\epost{\email{piero.mana}{ntnu.no}}}%
\hspace*{\stretch{1}}%
%\quad\href{https://orcid.org/0000-0002-6070-0784}{\protect\includegraphics[scale=0.16]{orcid_32x32.png}\textsc{orcid}:0000-0002-6070-0784}%
}

\date{Draft of \today\ (first drafted \firstdraft)}
%\date{\firstpublished; updated \updated}

%@@@@@@@@@@ new macros @@@@@@@@@@
% Common ones - uncomment as needed
%\providecommand{\nequiv}{\not\equiv}
%\providecommand{\coloneqq}{\mathrel{\mathop:}=}
%\providecommand{\eqqcolon}{=\mathrel{\mathop:}}
%\providecommand{\varprod}{\prod}
\newcommand*{\de}{\partialup}%partial diff
\newcommand*{\pu}{\piup}%constant pi
\newcommand*{\delt}{\deltaup}%Kronecker, Dirac
%\newcommand*{\eps}{\varepsilonup}%Levi-Civita, Heaviside
%\newcommand*{\riem}{\zetaup}%Riemann zeta
%\providecommand{\degree}{\textdegree}% degree
%\newcommand*{\celsius}{\textcelsius}% degree Celsius
%\newcommand*{\micro}{\textmu}% degree Celsius
%\newcommand*{\I}{\mathrm{i}}%imaginary unit
%\newcommand*{\e}{\mathrm{e}}%Neper
\newcommand*{\di}{\mathrm{d}}%differential
%\newcommand*{\Di}{\mathrm{D}}%capital differential
%\newcommand*{\planckc}{\hslash}
%\newcommand*{\avogn}{N_{\textrm{A}}}
%\newcommand*{\NN}{\bm{\mathrm{N}}}
%\newcommand*{\ZZ}{\bm{\mathrm{Z}}}
%\newcommand*{\QQ}{\bm{\mathrm{Q}}}
\newcommand*{\RR}{\bm{\mathrm{R}}}
\newcommand*{\CC}{\bm{\mathrm{C}}}
%\newcommand*{\nabl}{\bm{\nabla}}%nabla
%\DeclareMathOperator{\lb}{lb}%base 2 log
%\DeclareMathOperator{\tr}{tr}%trace
%\DeclareMathOperator{\card}{card}%cardinality
%\DeclareMathOperator{\im}{Im}%im part
%\DeclareMathOperator{\re}{Re}%re part
%\DeclareMathOperator{\sgn}{sgn}%signum
%\DeclareMathOperator{\ent}{ent}%integer less or equal to
%\DeclareMathOperator{\Ord}{O}%same order as
%\DeclareMathOperator{\ord}{o}%lower order than
%\newcommand*{\incr}{\triangle}%finite increment
\newcommand*{\defd}{\coloneqq}
\newcommand*{\defs}{\eqqcolon}
%\newcommand*{\Land}{\bigwedge}
%\newcommand*{\Lor}{\bigvee}
%\newcommand*{\lland}{\mathbin{\ \land\ }}
%\newcommand*{\llor}{\mathbin{\ \lor\ }}
%\newcommand*{\lonlyif}{\mathbin{\Rightarrow}}%implies
%\newcommand*{\limplies}{\mathbin{\Rightarrow}}%implies
\newcommand*{\mimplies}{\Rightarrow}%implies
%\newcommand*{\liff}{\mathbin{\Leftrightarrow}}%if and only if
%\newcommand*{\cond}{\mathpunct{|}}%conditional sign (in probabilities)
%\newcommand*{\lcond}{\mathpunct{|\ }}%conditional sign (in probabilities)
%\newcommand*{\bigcond}{\mathpunct{\big|}}%conditional sign (in probabilities)
%\newcommand*{\lbigcond}{\mathpunct{\big|\ }}%conditional sign (in probabilities)
\newcommand*{\suchthat}{\mid}%{\mathpunct{|}}%such that (eg in sets)
%\newcommand*{\bigst}{\mathpunct{\big|}}%such that (eg in sets)
%\newcommand*{\with}{\colon}%with (list of indices)
%\newcommand*{\mul}{\times}%multiplication
%\newcommand*{\inn}{\cdot}%inner product
%\newcommand*{\dotv}{\mathord{\,\cdot\,}}%variable place
%\newcommand*{\comp}{\circ}%composition of functions
%\newcommand*{\con}{\mathbin{:}}%scal prod of tensors
%\newcommand*{\equi}{\sim}%equivalent to 
\renewcommand*{\asymp}{\simeq}%equivalent to 
%\newcommand*{\corr}{\mathrel{\hat{=}}}%corresponds to
%\providecommand{\varparallel}{\ensuremath{\mathbin{/\mkern-7mu/}}}%parallel (tentative symbol)
\renewcommand{\le}{\leqslant}%less or equal
\renewcommand{\ge}{\geqslant}%greater or equal
%\DeclarePairedDelimiter\clcl{[}{]}
\DeclarePairedDelimiter\clop{[}{[}
%\DeclarePairedDelimiter\opcl{]}{]}
%\DeclarePairedDelimiter\opop{]}{[}
%\DeclarePairedDelimiter\abs{\lvert}{\rvert}
%\DeclarePairedDelimiter\norm{\lVert}{\rVert}
\DeclarePairedDelimiter\set{\{}{\}}
%\DeclareMathOperator{\pr}{P}%probability
\newcommand*{\pf}{\mathrm{p}}%probability
\newcommand*{\p}{\mathrm{P}}%probability
%\newcommand*{\tf}{\mathrm{T}}%probability
\renewcommand*{\|}{\mathpunct{|}}
%\newcommand*{\lcond}{\mathpunct{|\ }}%conditional sign (in probabilities)
\newcommand*{\bigcond}{\mathpunct{\big|\ }}%conditional sign (in probabilities)
%\newcommand*{\lbigcond}{\mathpunct{\big|\ }}%conditional sign (in probabilities)
%\newcommand*{\+}{\lor}
%\renewcommand{\*}{\land}
\newcommand*{\sect}{\S}% Sect.~
\newcommand*{\sects}{\S\S}% Sect.~
\newcommand*{\chap}{ch.}%
\newcommand*{\chaps}{chs}%
\newcommand*{\bref}{ref.}%
\newcommand*{\brefs}{refs}%
%\newcommand*{\fn}{fn}%
\newcommand*{\eqn}{eq.}%
\newcommand*{\eqns}{eqs}%
\newcommand*{\fig}{fig.}%
\newcommand*{\figs}{figs}%
\newcommand*{\vs}{{vs}}
\newcommand*{\etc}{{etc.}}
%\newcommand*{\ie}{{i.e.}}
%\newcommand*{\ca}{{c.}}
%\newcommand*{\eg}{{e.g.}}
\newcommand*{\foll}{{ff.}}
%\newcommand*{\viz}{{viz}}
\newcommand*{\cf}{{cf.}}
%\newcommand*{\Cf}{{Cf.}}
%\newcommand*{\vd}{{v.}}
\newcommand*{\etal}{{et al.}}
%\newcommand*{\etsim}{{et sim.}}
%\newcommand*{\ibid}{{ibid.}}
%\newcommand*{\sic}{{sic}}
%\newcommand*{\id}{\mathte{I}}%id matrix
%\newcommand*{\nbd}{\nobreakdash}%
%\newcommand*{\bd}{\hspace{0pt}}%
%\def\hy{-\penalty0\hskip0pt\relax}
%\newcommand*{\labelbis}[1]{\tag*{(\ref{#1})$_\text{r}$}}
%\newcommand*{\mathbox}[2][.8]{\parbox[t]{#1\columnwidth}{#2}}
%\newcommand*{\zerob}[1]{\makebox[0pt][l]{#1}}
\newcommand*{\tprod}{\mathop{\textstyle\prod}\nolimits}
\newcommand*{\tsum}{\mathop{\textstyle\sum}\nolimits}
%\newcommand*{\tint}{\begingroup\textstyle\int\endgroup\nolimits}
%\newcommand*{\tland}{\mathop{\textstyle\bigwedge}\nolimits}
%\newcommand*{\tlor}{\mathop{\textstyle\bigvee}\nolimits}
%\newcommand*{\sprod}{\mathop{\textstyle\prod}}
%\newcommand*{\ssum}{\mathop{\textstyle\sum}}
%\newcommand*{\sint}{\begingroup\textstyle\int\endgroup}
%\newcommand*{\sland}{\mathop{\textstyle\bigwedge}}
%\newcommand*{\slor}{\mathop{\textstyle\bigvee}}
%\newcommand*{\T}{^\intercal}%transpose
%\newcommand*{\E}{\mathrm{E}}
%\DeclarePairedDelimiter\expp{(}{)}
%\newcommand*{\expe}{\E\expp}%round
%\newcommand*{\expeb}{\E\clcl}%square
%%\newcommand*{\QEM}%{\textnormal{$\Box$}}%{\ding{167}}
%\newcommand*{\qem}{\leavevmode\unskip\penalty9999 \hbox{}\nobreak\hfill
%\quad\hbox{\QEM}}

\colorlet{notecolour}{mygreen}
%\newcommand*{\puzzle}{{\fontencoding{U}\fontfamily{fontawesometwo}\selectfont\symbol{225}}}
\newcommand*{\puzzle}{\maltese}
\newcommand{\mynote}[1]{ {\color{notecolour}\puzzle\ #1}}
%\newcommand*{\widebar}[1]{{\mkern1.5mu\skew{2}\overline{\mkern-1.5mu#1\mkern-1.5mu}\mkern 1.5mu}}

%\DeclareMathOperator*{\argsup}{arg\,sup}
\DeclareMathOperator*{\argmax}{arg\,max}

\newcommand{\explanation}[4][t]{%\setlength{\tabcolsep}{-1ex}
%\smash{
\begin{tabular}[#1]{c}#2\\[0.5\jot]\rule{1pt}{#3}\\#4\end{tabular}}%}

\newcommand*{\dob}{degree of belief}
\newcommand*{\dobs}{degrees of belief}
\newcommand*{\yp}{p}
\newcommand*{\ya}{a}
\newcommand*{\yg}{G}
\newcommand*{\yI}{\varIota}
\newcommand*{\yf}{\bm{f}}
\newcommand*{\xo}[1]{x^{(#1)}}
\newcommand*{\yo}[1]{y^{(#1)}}
\newcommand*{\yT}{T'}
\newcommand*{\yq}{\bm{q}}
\newcommand*{\yH}{D}
\newcommand*{\yN}{\varMu}
\newcommand*{\yD}{D}
\newcommand*{\yM}{\varMu}
\newcommand*{\yE}{E}
%@@@@@@@@@@ new macros end @@@@@@@@@@

\firmlists
\begin{document}
\captiondelim{\quad}\captionnamefont{\footnotesize}\captiontitlefont{\footnotesize}
\selectlanguage{british}\frenchspacing

%%% Title and abstract %%%
\maketitle
\abstractrunin
\abslabeldelim{}
\renewcommand*{\abstractname}{}
\setlength{\absleftindent}{0pt}
\setlength{\absrightindent}{0pt}
\setlength{\abstitleskip}{-\absparindent}
\begin{abstract}\labelsep 0pt%
  \noindent Notes on Bayesian and decision-theoretic interpretations of
  neural networks
\ifpublic\\\noindent\emph{\footnotesize Note: Dear Reader \amp\ Peer, this
  manuscript is being peer-reviewed by you. Thank you.}\fi
% \par%\\[\jot]
% \noindent
% {\footnotesize PACS: ***}\qquad%
% {\footnotesize MSC: ***}%
%\qquad{\footnotesize Keywords: ***}
\end{abstract}

\selectlanguage{british}\frenchspacing
% \asudedication{\small ***}
% \vspace{\bigskipamount}

% \setlength{\epigraphwidth}{.7\columnwidth}
% %\epigraphposition{flushright}
% \epigraphtextposition{flushright}
% %\epigraphsourceposition{flushright}
% \epigraphfontsize{\footnotesize}
% \setlength{\epigraphrule}{0pt}
% %\setlength{\beforeepigraphskip}{0pt}
% %\setlength{\afterepigraphskip}{0pt}
% \epigraph{\emph{text}}{source}

\section{Inference and decision}
\label{sec:intro}

First a summary about inference and decision, with some remarks of import
for the rest of these notes.

Let's call, somewhat improperly, \emph{inference}, \emph{prediction}, or
\emph{forecast} the process of assigning numerical values to our \dobs\
$\pf(\yp \|t, \yD, \yM)$ about a set of propositions $\set{\yp}$ given an
(in some problems optional) input $t$, data $\yD$ and a model $\yM$. The
propositions often concern the happening of events or the values of some
physical quantity $x$. The model is the set of numerical \dobs\ that are
sufficient to arrive at $\pf(\yp \|t, \yD, \yM)$ using the plausibility
calculus.

By \emph{decision} or \emph{choice} we call the process of choosing an
action among a set $\set{\ya}$. We must specify a gain function or matrix
$\yg(\ya \| \yp, \yM)$ that tells our gain in performing action $\ya$ when
the proposition $\yp$ holds. The gain function is part of our model.
According to decision theory
\citep{raiffaetal1961_r2000}[\chap~13]{jaynes1994_r2003} we should choose
one of the actions that maximize the expected gain:
\begin{equation}
  \label{eq:expected_gain}
  \text{choose}\quad
  \argmax_{\ya} \sum_{\yp} \yg(\ya \| \yp, \yM)\; \pf(\yp \| t, \yD, \yM).
\end{equation}
This decision problem can be rephrased in term of the maximum, rather than
the expectation, with respect to $\yp$
(\enquote{$\argmax_{\ya}\max_{\yp}$}), just by using a different gain
function. Decision problems always involve inference problems, but not vice
versa.

In the statistics literature a lot of effort is often spent on the
numerical choice and determination of \dobs, but little is spent on the
choice of the gain function. Yet it's clear from
formula~\eqref{eq:expected_gain} that the solution of the decision problem
depends equally on both.

Two models $\yM'$, $\yM''$ can lead to different inferences and yet be
identical for decision purposes if
\begin{multline}
  \label{eq:equivalent_decision_models}
  \sum_{\yp} \yg(\ya \| \yp, \yM')\; \pf(\yp \|t, \yD, \yM')
  =
  \sum_{\yp} \yg(\ya \| \yp, \yM'')\; \pf(\yp \|t, \yD, \yM'')
\\[-\jot]\text{for all $\ya$, $\yD$}.
\end{multline}
For example, when
$\pf(\yp \|t, \yD, \yM') = \pf(\yp \|t, \yD, \yM'')\times h(\yp)$ and
$\yg(\ya \| \yp, \yM') = \yg(\ya \| \yp, \yM'')/h(\yp)$. It's therefore
impossible to ascertain the \dobs\ determined by a model if we're only
given the decisions ensuing from it. This point will be important for our
Bayesian interpretations of machine-learning procedures.

Note the different nature of actions and propositions. Often the decision
problem is metonymically represented by the choice of an event or of the
value of a quantity, but the intended actions are different: we say
\enquote{choose $\yp$} but we mean \enquote{behave as if $\yp$ were true}.
For example: the uncertain events $\set{\yp}$ are
$\set{\text{\enquote{rainy}}, \text{\enquote{sunny}}}$ and we may represent
the actions $\set{\ya}$ by the same terms, but what we mean is actually
$\set{\text{\enquote{take the umbrella}}, \text{\enquote{don't take the
      umbrella}}}$. Often the literature presents problems that look like
inferences but are actually decisions. The \enquote{choice of a parameter}
is an example of this ambiguity.

Another ambiguity appears in the literature in the discussion of inference
problems. A model parameter is often presented as the object of our
inference, but our ultimate inference actually regards some more concrete
event. For example: we model our \dob\ about observations of a physical or
biologic quantity $x$ with a normal density, and the mean and variance of
the density are often presented as the objects of uncertainty; but our
ultimate uncertainty is about the value of $x$ in a new observation. The
parameters are secondary.

Both ambiguities above \citep[see][\sect~2 for some examples]{kassetal1995}
will also be important in our discussion of Bayesian interpretations of
machine-learning procedures.


\section{Synopsis of interpretations}
\label{sec:synopsis_interpretations}

I think it's important to first make a distinction between two tasks:
\begin{enumerate}[label=(\Alph*)]
\item\label{item:net_task}the general task that the neural network is meant
  to solve, once ready for use,
\item\label{item:train_task}the task of training the neural network.
\end{enumerate}
My impression is that the heuristics used in neural-net training address
both tasks. In fact, task~\ref{item:train_task} somehow involves
task~\ref{item:net_task}. This double, nested nature of neural-network
training is probably what makes it difficult to understand from a Bayesian
and decision-theoretical point of view.

The next question is whether each task above is an inference problem or a
decision problem. I think both tasks are decision problems.
For~\ref{item:net_task}, even in those cases where the network is designed
to report some measure of confidence, its main goal is to give a definite
output out of a set of possible ones. Think of examples like
optical-character recognition. For~\ref{item:train_task}, the software
engineer must in the end deliver a network, appropriately prepared, ready
to be used in an automated way. One complication in this question is
whether the ready-for-use neural network is meant to continue learning from
data given to it or not. It seems to me this isn't the case (think again of
optical-character recognition).

Let's now examine the two decision tasks above in more detail.

\smallskip

In task~\ref{item:net_task} the neural net must choose a specific output
$y$, given an input $t$. Such choice is an action; thus $y$ is a label for
an action, more than for a quantity. In optical-character recognition, for
example, $t$ is an array of pixel intensities of an image of text; the
uncertain proposition $\yp$ is about the typographic character $x$ that was
intended by the original writer of the text; and $y$ is the choice of a
specific character, say as a unicode number. There may of course be an
isomorphism between $\set{y}$ and $\set{x}$. This is an example of the
distinction between action and quantity mentioned in the previous section.
In this decision problem no additional data $\yD$ are used: the conditional
plausibilities $\pf(x \| t,\yM)$ are given and fixed. The model $\yM$ may
of course have been constructed or chosen using data, but these do not
enter the present task. For the neural-network user, this decision task
reduces to a specific function $f\colon t \mapsto y$, or possibly
$f\colon t \mapsto (y,c)$ where $c$ is a measure of confidence. This
function can be obtained by many different pairs of plausibility
distributions and gain functions; see the remarks in the previous section.

\smallskip

Task~\ref{item:train_task} can be seen from two points of view:
\begin{enumerate}[wide,label=(\arabic*)]
\item If we see task~\ref{item:net_task} as summarized by a function
  $f\colon t\mapsto y$, then the task of the software engineer is to choose
  such a function from a set $\set{f}$ of possible ones. To make this
  choice she needs a gain function $\yg(f \| \yp, \yE)$ and the set of
  plausibility distributions $\pf(\yp \| t,\yD, \yE)$, chosen according to
  some model $\yE$. But what are the propositions $\set{\yp}$ about? that
  is, what is the engineer uncertain about? Aind what is the gain about?

  The engineer is uncertain about which pairs $(t,x)$ the neural network
  will encounter during its actual use \citep[see][\sect~22.3 for an
  enlightening discussion about this]{jaynes1994_r2003}. We may distinguish
  two cases of such uncertainty: either there is a deterministic relation
  between the two quantities, that is, the engineer knows that only one $x$
  corresponds to each $t$, although she doesn't know which; or there isn't
  a deterministic relation, and the engineer is uncertain about which
  values of $x$ will appear for multiple occurrences of the same $t$.

  As for the gain in choosing $f$, my intuition is that it is the average
  gain that the end user of the neural net will obtain during its use. If
  $\yg(y \|x)$ is the gain function for the end user, and
  $\pf(x,t \| \yE)$ is the engineer's \dob\ about the $(t,x)$ pairs the end
  user will encounter, the expected gain of choosing a function $f\colon t
  \mapsto y$ is
  \begin{equation}
    \label{eq:engineer_gain}
    \sum_{x,t} \yg[f(t) \|x]\;\yp(x,t \| \yE),
  \end{equation}
  so the engineer should choose an $f$ which maximizes this expectation. If
  the engineer knows that there's a deterministic dependence between $t$
  and $x$, say $r\colon t \mapsto x$, but she's uncertain as to this
  function, then the expectation above may be rewritten
  \begin{equation}
    \label{eq:engineer_gain_deterministic}
    \sum_{r,t} \yg[f(t) \| r(t)]\;\yp(r \| \yE)\;\yp(t \| \yE),
  \end{equation}
  assuming that her knowledge of $t$ is irrelevant for her uncertainty
  about $r$.

  Data $\yD$ are useful to the engineer for arriving at better-informed
  \dob s about $\pf(x,t \| \yD, \yE)$. So, considering the
  expectation~\eqref{eq:engineer_gain}, the software engineer must 
  \begin{equation}
    \label{eq:choice_engineer}
    \text{choose}\quad
\argmax_{f}\sum_{x,t} \yg[f(t) \|x]\;\yp(x,t \| \yD, \yE).
  \end{equation}

  The heuristic procedures typical neural-net learning could be doing the
  following:
  \begin{itemize}
  \item calculating the posterior $\pf(x,t \| \yD, \yE)$,
  \item searching for the $f$ that maximizes the expectation above,
  \item both.
  \end{itemize}

  An important feature of the point of view currently under discussion is
  that there the space of functions $\set{f}$ can be distinct from the
  space of plausibility models $\pf(x,t \| \yD, \yE)$ or the space of
  functions $\set{r}$. This is in contrast with the Bayesian interpretation
  most often discussed in the literature
  \citep{tishbyetal1989,levinetal1990,mackay1992,mackay1992b,neal1996,barberetal1998}[esp.
  \sects~1.2--3, 3.3--4, 5.7]{bishop2006}, which assumes a neural network
  to be a particular plausibility model.
\end{enumerate}

\mynote{To be continued}




\clearpage
\hrule
old text
\hrule

The purpose of a neural network and some other machine-learning methods is
to choose an output $y$ given an input $x$ and data relating the possible
inputs and outputs. The way this choice is made and the way it is
influenced by already-observed data make intuitive sense. As stated, the
task above involves uncertainty and decision; it can therefore also be
approached methodically using the plausibility calculus -- the
\enquote{gold standard} as Srivastava, Hinton, \etal\
\citey{srivastavaetal2014} called it -- and the principles of decision
theory. It is interesting to see how the intuitive neural-net approach and
the methodical one relate to each other.

This relation was first studied by Tishby, Levin, Solla
\citep{tishbyetal1989,levinetal1990} and then brilliantly explored by
MacKay \citey{mackay1992,mackay1992b} and others
\citey{neal1996,barberetal1998}.

\section{***}





The study and grasp of the connection between neural nets and probability
is beneficial to both. For neural nets, this connection \emph{might} not
lead to any practical improvements -- because of scalability \etc*** -- but
it's very useful for understanding open problems, like the quantification
of confidence of neural-network outputs [refs***], overtraining, or the
choice of architectural constants. For example, the successful idea of
using \enquote{dropout} was also motivated
\citep{hintonetal2012,srivastavaetal2014} by the \enquote{mixing} of
predictions typical of probability. For probability, this connection
suggests new, efficient parametric families and approximation methods.

\medskip

In this note I want to explain the connection between neural nets and some
probability models. What's new in this note, compared with the 1990s
studies cited above?
\begin{itemize}
\item I want to try to show this connection in a \emph{geometric, visual}
  way.
\item I extend those studies in two directions:
  \begin{enumerate}[label=\arabic*.]
  \item including the decision-theoretic character of the problem. This
    leads to a different probabilistic interpretation of the error function
    of neural nets;
  \item basing the connection on so-called \enquote{partially exchangeable
      models}, to be discussed later. They give us a better understanding
    of overtraining, and are also connected with generative models
    [refs***].
  \end{enumerate}
\item\emph{Repetita iuvant}: knowledge of those studies seems to be
  forgotten or much diluted today.
\end{itemize}

\medskip

This is what I do not purport to do:
\begin{itemize}[label=${}-{}$]
\item proving that the connection between neural nets and probability
  discussed here is unique. Even if it isn't unique, it's still insightful and
  useful;
\item suggest that neural nets should be used \enquote{more in accordance}
  with the probability calculus. The connection already shows that they are
  approximate probability calculations; their speed and power come from
  their approximate nature;
\item\ [add here whatever else you'd like my intentions not to be.]
\end{itemize}



\section{Introduction to partially exchangeable models}



Consider a classification problem: given input
$x \in \set{1,\dotsc, M}$, we want to predict output
$y \in \set{1,\dotsc,N}$. Let's consider both discrete.

Two cases must be distinguished: whether every input has several possible
outputs, or just one. The second case is a special instance of the first,
and could be simply treated as the prediction of a \enquote{function}
$x\mapsto y$. But both cases are instances of inference with a
\emph{partially exchangeable} model.


Our assumptions are summarized in the proposition $\yI$. We assume that the
probability of $y$, for each kind of input $x$, is exchangeable. Thus the
probability for several pairs
\begin{equation}
  \label{eq:prob_several_pairs}
  \pf(\yo{T},\dotsc, \yo{1} \|\xo{T}, \dotsc, \xo{1}, \yI)
\end{equation}
must have a partially exchangeable form [refs]: for every kind of input
$x$ we consider the $N$ relative frequencies
$\yf_x \defd (f_{1\|x},\dotsc, f_{N\|x})$ of the $N$ possible outputs. The
probability above is given by this integral:
\begin{multline}
  \label{eq:prob_several_pairs_exch}
  \pf(\yo{T},\dotsc, \yo{1} \|\xo{T}, \dotsc, \xo{1}, \yI)
  ={}\\
  \idotsint
  \biggl[ \prod_{x=1}^M \prod_t^{\xo{t}=x} f_{\yo{t}\|x} \biggr]
  \;
  \pf(\yf_1,\dotsc,\yf_M \| \yI)
  \;\di\yf_1 \dotsm \di\yf_M.
\end{multline}

The complicated look of this formula doesn't do justice to its simple and
intuitive interpretation:

Choose a kind of input $x$. We're judging that the probability for a very
large sequence of outputs generated with this input doesn't depend on their
order. Suppose we knew the relative frequencies of each kind of output in
this sequence, $(f_{1\|x},\dotsc,f_{N\|x}) \defs \yf_x$, and knew nothing
else. Then out of symmetry reasons we should give a probability $f_{y\|x}$
of observing outcome $y$ at the next observation. This is just a
\enquote{drawing without replacement} problem. At the subsequent
observations with the same input we give again the same probabilities to
each $y$, because the sequence is so large that we approximate
\enquote{drawing without replacement} with \enquote{drawing with
  replacement}. The probability for a sequence of outputs
$\yo{1},\dotsc,\yo{T}$ from the same input $x$ is then
\begin{equation}
  \pf(\yo{T}, \dotsc,\yo{1}\|\text{\footnotesize same input $x$},\yf_x, \yI) =
  f_{\yo{T}\|x}\times\dotsm\times f_{\yo{1}\|x}.
  \label{eq:input_m_indep_output}
\end{equation}
Within a larger sequence of outputs from all possible inputs, the outputs
$\yo(t)$ coming from input $x$ are those for which $\xo(t)=x$. Thus we can
write the formula above more generally as
\begin{equation}
 \prod_t^{\xo{t}=x} f_{\yo{t}\|x}
  \label{eq:input_m_indep_output_label_x}
\end{equation}
The above reasoning applies for each kind of input $x$. Thus the
probability for a sequence of outputs coming from different inputs is the
product of the probabilities above for all different $x$:
\begin{equation}
  \pf(\text{\footnotesize all outputs} \|\text{\footnotesize inputs}, \yf_1,\dotsc,\yf_M, \yI) =
\prod_{x=1}^M \prod_t^{\xo{t}=x} f_{\yo{t}\|x}.
  \label{eq:input_m_indep_output_all_x}
\end{equation}


Now what if we don't know the relative frequencies $\yf_x$, for any input?
Then we assign a probability distribution over their possible values% ,
% $\pf(\yf_1,\dotsc,\yf_M \| \yI)\,\di\yf_1\dotsm\di\yf_M$,
and use the law of total probability:
\begin{multline}
  \label{eq:freqs_total_prob}
  \pf(\text{\footnotesize all outputs} \|\text{\footnotesize inputs}, \yI) ={}\\
  \idotsint \pf(\text{\footnotesize all outputs} \|\text{\footnotesize inputs},
  \yf_1,\dotsc,\yf_M, \yI)
  \; \pf(\yf_1,\dotsc,\yf_M \| \yI)\,\di\yf_1,\dotsc,\di\yf_M.
\end{multline}
Substituting the explicit expression~\eqref{eq:input_m_indep_output_all_x}
in this formula we obtain formula~\eqref{eq:freqs_total_prob}. In summary,
\begin{equation}
  \label{eq:prob_several_pairs_exch_explanation}
  \textcolor{mygreen}{\idotsint}
  \biggl[ \textcolor{mypurpleblue}{\explanation{$\displaystyle\prod_{x=1}^M$}{3em}{\clap{\parbox{10em}{\centering\footnotesize product over all inputs}}}} \textcolor{myred}{\explanation{$\smash{\displaystyle\prod_t^{\xo{t}=x} f_{\yo{t}\|x}}$}{0.67em}{\clap{\quad\parbox[t]{8em}{\centering\footnotesize probability for outputs\\from same input}}}} \biggr]
  \;
  \textcolor{mygreen}{\explanation{$\pf(\yf_1,\dotsc,\yf_M \| \yI)
    \,\di\yf_1 \dotsm \di\yf_M$}{3em}{\clap{\parbox{12em}{\centering\footnotesize uncertainty over all frequencies}}}}.
\end{equation}


The possible frequencies give one input,
$\yf_x \equiv (f_{1\|x}, \dotsc, f_{N\|x})$, belong to the
$(N-1)$-dimensional simplex
\begin{equation}
  \label{eq:simplex_N}
  \varDelta \defd \set[\big]{(f_1, \dotsc,f_N) \suchthat f_i \ge 0, \tsum_i f_i =1},
\end{equation}
and the collection of possible frequencies $(\yf_1,\dotsc, \yf_M)$ belongs
to the $M$-fold Cartesian product $\varDelta^M$. From now on we denote
$\yf \defd (\yf_1,\dotsc,\yf_M)$.

\bigskip

The probability for a new sequence of $\yT$ outputs given their inputs and
given that we've learned a previous sequence of $T$ input-output pairs is
determined by Bayes's theorem:
\begin{equation}
  \label{eq:prob_new_several_pairs}
  \begin{gathered}
  \pf(\yo{\yT},\dotsc, \yo{T+1} \|\xo{\yT}, \dotsc, \xo{T+1},\;
  \yo{T},\xo{T},\dotsc, \yo{1},\xo{1},\; \yI) ={}\\
  \frac{\displaystyle
    \int
  \biggl[ \prod_{x=1}^M\, \prod_{t=T+1,\dotsc,\yT}^{\xo{t}=x} f_{\yo{t}\|x} \biggr]
  \;  \pf(\yf \| \yI)  \;\di\yf
}{\displaystyle
\int
  \biggl[ \prod_{x=1}^M\, \prod_{t=1,\dotsc,T}^{\xo{t}=x} f_{\yo{t}\|x} \biggr]
  \;  \pf(\yf \| \yI)  \;\di\yf
}.
\end{gathered}
\end{equation}
This formula is equivalent to~\eqref{eq:prob_several_pairs_exch} with and
updated distribution for the frequencies:
\begin{equation}
  \label{eq:update_pr_freqs}
  \begin{gathered}
  \pf(\yf \| \yo{T},\xo{T},\dotsc, \yo{1},\xo{1},\; \yI) \;\di\yf={}\\
  \frac{
  \bigl[ \prod_{x=1}^M\, \prod_{t=1,\dotsc,T}^{\xo{t}=x} f_{\yo{t}\|x} \bigr]
  \;  \pf(\yf \| \yI) 
}{
  \int  \bigl[ \prod_{x=1}^M\, \prod_{t=1,\dotsc,T}^{\xo{t}=x} f_{\yo{t}\|x} \bigr]
  \;  \pf(\yf \| \yI) \;\di\yf
  } \;\di\yf.
\end{gathered}
\end{equation}

The form of this updated distribution has important consequences for our
learning process.

If the number of learned data is enough large compared with the numbers
$M$, $N$ of possible inputs and outputs and with the magnitude of the
initial distribution for the frequencies, and if the latter is strictly
positive, then the updated distribution becomes very peaked on the
collection of relative frequencies $(\yq_1,\dotsc,\yq_M)$ of the learned
outputs for all input values. This can be seen from the asymptotic
expression in terms of the relative entropy (Kullback-Leibler divergence)
$\yH$,
  \begin{equation}
    \label{eq:asympt_pf_KL}
    \pf(\yf \| \yo{T},\xo{T},\dotsc, \yo{1},\xo{1},\; \yI)
    \propto
  \exp\bigl[ -\tsum_x T_x\yH(\yq_x \| \yf_x)  \bigr]\;
    \pf(\yf \| \yI),
  \end{equation}
where $T_x$ is the number of observations with input $x$, with $\sum_x T_x=T$.

If the number of learned data is small compared with the dimensions of the
input and output spaces, then the initial distribution for the frequencies
$\pf(\yf \| \yI)\,\di\yf$ greatly influence our
inference~\eqref{eq:prob_new_several_pairs}. This distribution determines
two important ***



\section{Utility functions and probabilities}
\label{sec:utilities_probs}

Utility of behaving as if proposition $B \in X$ is true given that proposition
$A \in X$ is true: $c(B \|A)$. Probability for $A$ given $D,\yI$: $\p(A \|
D, \yI)$. Optimal decision is $B$ that maximizes
\begin{equation}
  \label{eq:optimal_decision}
  \sum_A c(B \| A)\, \p(A \|D,\yI).
\end{equation}

Now consider different probabilities for all $A$ given the same data $D$:
$\p(A\|D, \yI')$. The decision $B$ will still be the same if we use a new
utility
\begin{equation}
  \label{eq:new_utility}
  c'(B \| A) \defd c(B \|A)\,\frac{\p(A \|D,\yI)}{\p(A \|D,\yI')}.
\end{equation}
So the same choice can be made with a different probability, if the utility
is appropriately changed, provided $p(A \|D,\yI')>0$ for all $A$.

This leads to a slightly more general view than Tishby \etal's
\citey{tishbyetal1989,levinetal1990} and Mackay's \citey{mackay1992,mackay1992b}



\mynote{Point out that the joint parameter density allows us to make
  inferences 1. from data about one output to data about the same output;
  2. from data about one output to data about a different output --
  \emph{this} is generalization}

\mynote{Point out that generalization (from one output to another) is
  \emph{fully} determined by the form of the joint parameter prior}






\newpage


\textcolor{white}{If you find this you can claim a postcard from me.}
%

\setlength{\intextsep}{0.5ex}% with wrapfigure
%\begin{figure}[p!]%{r}{0.4\linewidth} % with wrapfigure
%  \centering\includegraphics[trim={12ex 0 18ex 0},clip,width=\linewidth]{maxent_saddle.png}\\
%\caption{***}\label{fig:comparison_a5}
%\end{figure}% exp_family_maxent.nb


\iffalse
\begin{acknowledgements}
  \ldots to Mari \amp\ Miri for continuous encouragement and affection, and
  to Buster Keaton and Saitama for filling life with awe and inspiration.
  To the developers and maintainers of \LaTeX, Emacs, AUC\TeX, Open Science
  Framework, Python, Inkscape, Sci-Hub for making a free and unfiltered
  scientific exchange possible.
%\rotatebox{15}{P}\rotatebox{5}{I}\rotatebox{-10}{P}\rotatebox{10}{\reflectbox{P}}\rotatebox{-5}{O}.
\sourceatright{\autanet}
\end{acknowledgements}
\fi

\renewcommand*{\appendixpagename}{Appendix}
\renewcommand*{\appendixname}{Appendix}
%\appendixpage
%\appendix

%%%%%%%%%%%%%%% BIB %%%%%%%%%%%%%%%

\defbibnote{prenote}{{\footnotesize (\enquote{de $X$} is listed under D,
    \enquote{van $X$} under V, and so on, regardless of national
    conventions.)\par}}
% \defbibnote{postnote}{\par\medskip\noindent{\footnotesize% Note:
%     \arxivp \mparcp \philscip \biorxivp}}

\printbibliography[prenote=prenote%,postnote=postnote
]


\end{document}
---------- cut text ----------------


%%% Local Variables: 
%%% mode: LaTeX
%%% TeX-PDF-mode: t
%%% TeX-master: t
%%% End: 
